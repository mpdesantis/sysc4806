# README
* Author: Michael De Santis
* CUID: 101213450
* Date: 2024/11/18

## Reading Quiz: Scalability and Virtualization

### 1. What is chroot, and why is this relevant to lightweight virtualization?

`chroot` is a UNIX utility (command) that, as per the manual page, allows a user to run "a command or interactive shell with a special root directory." Its name originates, like other UNIX utilities, by using the "ch" of "change" prepended to what it is changing (ie. "root"), meaning that you can change the root directory for a given command or session. By default, most UNIX filesystems take their root as the `/` root directory, and rely on this convention when searching and using the filesystem. By changing this default through the use of a tool like `chroot`, commands or sessions can set a given root directory to replace `/` on filesystem reads, writes, and executes. In doing so, the command or session can be isolated from the default system, protecting the root filesystem and providing additional freedoms in the "`chroot` jail" environment that is created. For example, if you have a binary `/usr/bin/echo` on your root filesystem, but you want the `echo` command to execute the binary at `/home/myuser/usr/bin/echo` instead, you could could `chroot` to `/home/myuser/` instead of `/`.

`chroot` is a handy utility if you want to experiment with programs in a UNIX environment, but do not want to break your root filesystem. For example, if you are cross-compiling for non-native architecture, you may want to use `chroot` to make it easier for your compilation toolchain to find the libraries of your target architecture, rather than searching for them in your host architecture root file system. `chroot` was the initial basis for a host of isolation technologies that has most recently come to be known as containers. `chroot` is very lightweight in the sense that it doesn't really require much overhead in offering its virtualization - essentially, at its core, it is just a virtual path redefinition that changes the execution environment from the default system environment. Containers seek to offer the same benefits as `chroot` while maintaining the lightweight quality, but with even more functionality - such as portability. Even with lightweight containers available, `chroot` still finds use in lots of development environments today, and even in deployed systems (I've used it at work to manage certain processes on embedded Linux devices!).


### 2. What does server "provisioning" mean?

Server "provisioning" is the process by which a server is allocated resources, configured, and brought up before use. Basically, it involves all the setup of a server in terms of hardware, software, and connectivity before the server can be used to meet its assigned purposed. As per Shiv Nagarajan, provisioning is "how you apply configuration" to a server. It involves allocating the actual hardware that the server will run on (this can be a single machine or distributed across machines), as well as the compute power that will be given to the server (eg. type and number of cores, memory, NICs, etc.). It also includes configuring the server for use, and situating it within the appropriate network where it will serve, ensuring it has access to the resources and connections it will need. If a server is "bare metal", the provisioning may include purchasing the server, configuring it for use, installing it physically in some location, and switching it into a network. If a server is virtualized or software-defined, the provisioning may be performed by providing configuration to specify the resources, compute, and connections the virtual server will need to be configured with before use. Many Infrastructure as a Service (IaaS) providers can perform server provisioning on demand, allowing customers to quickly spin up and spin down machines to match their load (this is known as elastic provisioning).


### 3. What does Chef do?
Chef is a configuration management tool that is used to assist in the provisioning of systems, typically servers. Chef takes "recipes" for system configuration, and then applies that configuration in the provisioning of a system. Chef, as well as other tools like it (such as the Python and SSH based Ansible) help to automate many of the tedious tasks often associated with configuring a server, and programmatically provides a mechanism by which many systems can be uniformly and equivalently configured in parallel. This capability makes Chef an essential tool at scale, as it allows for many identical servers to be spun up simultaneously without repeating manual efforts (just apply the same recipe to `n` many servers). Additionally, by programmatically applying configuration, the chance of error or discrepancy in applying the configuration is greatly reduced.

### 4. What are containers? What are some advantages of using containers for SaaS?
Containers are a lightweight virtualization solution that provide isolated environments for process execution. Containers trace their history back to `chroot` (mentioned above), through to LXC, Docker, Podman, and the Open Container Initiative (OCI). Containers are essentially a "contained" process or program that is packaged with all of its dependencies in a root filesystem that may be executed in isolation from the server machine that hosts it. Linux containers are so lightweight because, as opposed to other virtualization solutions like a Virtual Machine (VM), they do not include an Operating System (OS) level. Linux containers may be executed on any machine with an appropriate version of the Linux kernel (including, tragically, the Windows Subsystem for Linux (WSL) - yuck!). Additionally, containers can even be used across machines of different architectures by using a tool like QEMU to provide multi-arch support. Thus, containers are also highly portable - they really just need a Linux kernel and a container runtime on the host system.

Containers are executed using a container runtime (eg. `runc` or `crun`), which is typically wrapped by a higher level management tool, such as Docker or Podman. At runtime, they offer a high level of isolation in terms of filesystem access (such as in a `chroot` jail), but also in terms of resource consumption, as they are executed by the underlying host OS as a process, and as such can be constrained in terms of CPU and memory (by using `cgroups`, for example). Multiple containers can be executed simultaneously on a system (they are just processes), allowing a host device, often with the help of a container orchestration tool, to elastically provision containers as needed, spinning them up and down with demand. Containers are an excellent tool in Software as a Service (SaaS) applications, as many identical container instances corresponding to a service can be spun up (from a container image locally held or retrieved from a remote container repository) as they are needed - and only as they are needed. Correspondingly, they can also be spun down or blown away when they are no longer needed. Containers offer isolation, uniformity, and scalability in a portable and lightweight way. 

Containers are also super handy in embedded Linux systems - this is how I make my money!

P.S. I love containers.

### 5. <insert tech> is to groups of containers what Chef is to servers.

A container orchestration tool, such as Docker Swarm, Kubernetes (k8s, k3s), handles the configuration management and provisioning of containers just as Chef handles these processes for servers. Containers are typically grouped into units called "pods", which are logical and not physical units (ie. they do not need to be run on the same metal, they can be distributed). Pods are typically monitored for health by the chosen container orchestration tool, and this tool can revive any containers that go down, or switch to a highly available replica of the container.

### 6. How can you use Load Shedding to deal with a sudden spike in web traffic?

Load shedding is a load control mechanism employed when a system is overloaded, such as may occur when there is a spike in web traffic to the system (for example, Black Friday at Shopify!). Load shedding relies on the monitoring of system capacity, and is activated when the overload occurs. Essentially, load shedding is just an active decision of the system to not service some portion of its load (ie. not service some of its requests or processes). Typically, this is done in a structured and deliberate way, such as in priority-aware shedding. In this case, requests are prioritized by some criteria, and placed into "buckets" according to their priority. Requests are dropped from the lowest priority bucket until either the load is sufficiently reduced or the bucket is completely empty; if the latter occurs, the same process is repeated at the next-lowest priority bucket. In this way, the system can intelligently dedicate its resources to high priority requests, and achieve an acceptable quality of service even if lower priority requests are degraded.

### 7. What are some common means to measure system capacity, and how are they calculated?

One common metric for capacity is utilization, which is a ratio of usage to total capacity of the system (ie. a usage percentage). To calculate this, you form the ratio by dividing the system's usage over its total capacity. 

Another metric for capacity is the exit rate of a system, or how many requests you can serve per unit of time (for example, requests per second or RPS). To calculate this, you divide the number of requests capable of being served by the system over a defined interval of time to get the ratio of requests served per time unit.

A third metric for measuring system capacity is service time, or the average response time for a request to be serviced. This is typically expressed as latency, or the average amount of time between the issuance of a request and its service.

### 8. What is the difference between horizontal and vertical scaling?

Horizontal scaling typically refers to adding more instances of some resource in order to meet higher demands; for example, if a service's capacity is almost exhausted, more servers can be added as nodes in the system to increase capacity. Vertical scaling typically refers to increasing the compute power of existing nodes in a system in order to meet higher demands; for example, if a service's capacity is almost exhausted, individual servers can be upgraded (eg. better hardware components, more cores, more memory, better firmware or software, etc.) in order to increase system capacity. So, horizontal scaling means adding more servers, while vertical scaling means upgrading existing servers.

### 9. What are two places where caching can be used?
Caching, discussed as a capacity technology and not a performance technology, allows requested read-only data to be retrieved without database access - if that data is in the cache (a cache hit). Using the analogy of physical CPU caches, caching can occur at two different places between a client requesting data and the server providing it.

The cache may be between the server and the database, which allows the server to check the cache for the data specified in a received client request; if found in cache, then the server can forego the database access, saving the time, resource, and cost associated with the (typically slow) process of a database read. In the physical cache analogy, the service cache here acts like an L3 cache that is up a strata and accessible to all cores.

The cache may also be much closer to the client end-user, which can allow a cache hit to serve requested data without even having to transmit it from the server. For example, as mentioned by Derek Stride, Shopify maintains caches at Points of Presence (PoPs), where incoming requests for some geographical region are typically aggregated before being transported over a network spine. If a cache hit occurs at this PoP, then the data not only does not require read from a database, but also does not need to be sent from the server - it can be delivered from the PoP cache. In the physical analogy, this is more like the L2 strata of cache that sits on top of each core. This was one of the most interesting things about Derek's presentation - I did not think this was possible!

### 10. What is database sharding? How do you identify which shard has the target data that someone is looking for?

Database sharding is the process by which data is broken into segments (the "shards") and distributed across discrete databases. Essentially, it allows for multiple databases to act in concert with each other to each host shards of the combined data store. By intelligently distributing data in shards across multiple databases, this helps balance the load on each individual database, as it now only has to service a subset of the data (and not all of it). This increase in capacity and efficiency comes at the cost of increased complexity, as there must now be some means by which data requests are directed towards the correct shard. To handle this, the sharded database relies on a shard routing table. This routing table operates like a network routing table, except instead of specifying routes from one network node to another, it specifies the route to the database shard containing the target data. Since the shard routing table is consulted for each request, this needs to be kept up to date to ensure requests are routed to the correct shard. If some data is transferred from one shard to another (in an atomic transaction), this routing table should be updated after the transfer but before releasing the lock on the transacting systems, so that once the lock is released the routing table is already accurate for use. 


Thanks! Very interesting guest lectures.
